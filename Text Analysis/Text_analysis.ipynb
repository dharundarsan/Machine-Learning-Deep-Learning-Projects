{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wT4D4i3TK3D"
      },
      "outputs": [],
      "source": [
        "!pip install bs4\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB28cXgkTVwn",
        "outputId": "241ffd9a-2d71-42b9-f7aa-a5d94016dd77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import the dependencies\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "vZ14hIiK-Z7L"
      },
      "outputs": [],
      "source": [
        "# read the csv file\n",
        "df = pd.read_csv(\"input_file.csv\")\n",
        "\n",
        "# assign all the urls in a single variable\n",
        "urls = df['url']\n",
        "\n",
        "#index variable\n",
        "link = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "IvBYdRD-HXPi"
      },
      "outputs": [],
      "source": [
        "column_names = [\n",
        "    'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
        "    'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH',\n",
        "    'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',\n",
        "    'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT',\n",
        "    'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'\n",
        "]\n",
        "\n",
        "# allocating the outplut variables column to 0 initially\n",
        "for name in column_names:\n",
        "  df[name] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "GbEwScLsgvmH"
      },
      "outputs": [],
      "source": [
        "# stop words\n",
        "stop_words = []\n",
        "\n",
        "with open(\"StopWords_Auditor.txt\", \"r\") as f:\n",
        "    temp = f.read()\n",
        "\n",
        "stop_words = temp.split()\n",
        "\n",
        "\n",
        "with open(\"StopWords_Currencies.txt\", \"r\", encoding=\"ISO-8859-1\") as f:\n",
        "    temp = f.read()\n",
        "    stop_words += temp.split()\n",
        "    stop_words = [i for i in stop_words if i != \"|\"]\n",
        "\n",
        "\n",
        "\n",
        "with open(\"StopWords_DatesandNumbers.txt\", \"r\") as f:\n",
        "    temp = f.read()\n",
        "    stop_words += temp.split()\n",
        "    stop_words = [i for i in stop_words if i != \"|\"]\n",
        "\n",
        "\n",
        "with open(\"StopWords_Generic.txt\", \"r\") as f:\n",
        "    temp = f.read()\n",
        "    stop_words += temp.split()\n",
        "    stop_words = [i for i in stop_words if i != \"|\"]\n",
        "\n",
        "with open(\"StopWords_GenericLong.txt\", \"r\") as f:\n",
        "    temp = f.read()\n",
        "    stop_words += temp.split()\n",
        "    stop_words = [i for i in stop_words if i != \"|\"]\n",
        "\n",
        "with open(\"StopWords_Geographic.txt\", \"r\") as f:\n",
        "    temp = f.read()\n",
        "    stop_words += temp.split()\n",
        "    stop_words = [i for i in stop_words if i != \"|\"]\n",
        "\n",
        "\n",
        "with open(\"StopWords_Names.txt\", \"r\") as f:\n",
        "    temp = f.read()\n",
        "    stop_words += temp.split()\n",
        "    stop_words = [i for i in stop_words if i != \"|\"]\n",
        "\n",
        "# successfully all the stop words are collected in a single list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "LUQd2q3TVTgU"
      },
      "outputs": [],
      "source": [
        "def text_analysis(text, stop_words):\n",
        "  from nltk.tokenize import sent_tokenize\n",
        "\n",
        "  #tokenizing the sentence\n",
        "  token_sent = sent_tokenize(text)\n",
        "\n",
        "  from nltk import tokenize\n",
        "\n",
        "  #tokenizing the words\n",
        "  token_words = tokenize.word_tokenize(text)\n",
        "\n",
        "  # cleaning words - removing the stop words from token words\n",
        "\n",
        "  words_after_cleaning = []\n",
        "\n",
        "  stop_words = [i.lower() for i in stop_words]\n",
        "\n",
        "  for word in token_words:\n",
        "    if word not in stop_words:\n",
        "      words_after_cleaning += [word]\n",
        "\n",
        "  # cleaning sentences - removing stop words from each sentence\n",
        "\n",
        "  sentence_after_cleaning = []\n",
        "\n",
        "  for word in token_sent:\n",
        "    temp = \"\"\n",
        "    sent = word.split()\n",
        "    for w in sent:\n",
        "      if w not in stop_words:\n",
        "        temp += \" \" + w\n",
        "    sentence_after_cleaning.append(temp)\n",
        "\n",
        "  #positive words\n",
        "\n",
        "  with open(\"positive_words.txt\", \"r\") as f:\n",
        "      positive_list = f.read()\n",
        "\n",
        "  positive_list = positive_list.split()\n",
        "\n",
        "  #negative words\n",
        "\n",
        "  with open(\"negative-words.txt\", \"r\", encoding=\"ISO-8859-1\") as f:\n",
        "      neg_list = f.read()\n",
        "\n",
        "  negative_list = neg_list.split()\n",
        "\n",
        "  #positive score - number of positive words present in the token words\n",
        "\n",
        "  positive_set = set(positive_list)\n",
        "\n",
        "  positive_score = 0\n",
        "\n",
        "  token_words = [i.lower() for i in token_words]\n",
        "\n",
        "  for word in positive_set:\n",
        "    if word.lower() in token_words:\n",
        "      positive_score += 1\n",
        "\n",
        "  #negative score -  number of negative words present in the token words\n",
        "\n",
        "  negative_set = set(negative_list)\n",
        "\n",
        "  negative_score = 0\n",
        "\n",
        "  for word in negative_set:\n",
        "      if word.lower() in token_words:\n",
        "        negative_score += 1\n",
        "\n",
        "  #polarity score\n",
        "\n",
        "  polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "\n",
        "  total_words_after_cleaning = 0\n",
        "\n",
        "\n",
        "  # removing the punctuations\n",
        "\n",
        "  punctuations = [\".\", \";\", \":\", \",\", \"-\", \"`\", \"~\", \"!\", \"?\", \"'\", \"â€™\"]\n",
        "  words_after_cleaning = [i for i in words_after_cleaning if i not in punctuations]\n",
        "\n",
        "  total_words_after_cleaning = len(words_after_cleaning)\n",
        "\n",
        "  # subjectivity score\n",
        "\n",
        "  subjectivity_score = (positive_score - negative_score) / ((total_words_after_cleaning) + 0.000001)\n",
        "\n",
        "\n",
        "  # syllable count in a word\n",
        "\n",
        "  import nltk\n",
        "  from nltk.tokenize import SyllableTokenizer\n",
        "\n",
        "  # Download the nltk data needed for the syllable tokenizer\n",
        "\n",
        "  def count_syllables_nltk(word):\n",
        "      syllable_tokenizer = SyllableTokenizer()\n",
        "      syllables = syllable_tokenizer.tokenize(word)\n",
        "      return len(syllables)\n",
        "\n",
        "  def count_syllables_in_sentence_nltk(sentence):\n",
        "      words = nltk.word_tokenize(sentence)\n",
        "      total_syllables = sum(count_syllables_nltk(word) for word in words)\n",
        "      return total_syllables\n",
        "\n",
        "  def syllable_count(word):\n",
        "    count = count_syllables_nltk(word)\n",
        "\n",
        "    # handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable.'\n",
        "    if len(word) > 2:\n",
        "      if word[-2] + word[-1] == \"ed\" or word[-2] + word[-1] == \"es\":\n",
        "        count -= 1\n",
        "\n",
        "      # a silent 'e' at the end of a word doesn't contribute to an additional syllable.\n",
        "      if word[-1] == \"e\":\n",
        "        count -= 1\n",
        "\n",
        "    return count\n",
        "\n",
        "\n",
        "  complex_words = [i for i in words_after_cleaning if syllable_count(i) > 2]\n",
        "\n",
        "  # average sentence length\n",
        "\n",
        "  average_sentence_length = len(words_after_cleaning) / len(sentence_after_cleaning)\n",
        "\n",
        "  # complex word percentage\n",
        "\n",
        "  complex_word_percent = (len(complex_words) / len(words_after_cleaning)) * 100\n",
        "\n",
        "  # fog index\n",
        "\n",
        "  fog_index = (average_sentence_length + complex_word_percent) * 0.4\n",
        "\n",
        "  #average number of word per sentence\n",
        "\n",
        "  average_number_words_per_sentence = len(token_words) / len(token_sent)\n",
        "\n",
        "  # complex word count\n",
        "\n",
        "  complex_word_count = len(complex_words)\n",
        "\n",
        "  # word count\n",
        "\n",
        "  word_count = len(words_after_cleaning)\n",
        "\n",
        "  # syllable count per word\n",
        "\n",
        "  count_s = 0\n",
        "  for i in token_words:\n",
        "    count_s += syllable_count(i)\n",
        "\n",
        "  syllable_count_per_word = count_s / len(token_words)\n",
        "\n",
        "  #pronoun count\n",
        "\n",
        "  pronouns = [\"I\", \"we\", \"ours\", \"my\", \"us\"]\n",
        "\n",
        "  pronoun_count = 0\n",
        "\n",
        "  for pronoun in pronouns:\n",
        "    for word in token_words:\n",
        "      if word == pronoun:\n",
        "        pronoun_count += 1\n",
        "\n",
        "  # average word length\n",
        "\n",
        "  char_length = 0\n",
        "\n",
        "  for word in token_words:\n",
        "    char_length += len(word)\n",
        "\n",
        "  average_word_length = char_length / len(token_words)\n",
        "\n",
        "  return positive_score,negative_score,polarity_score,subjectivity_score,average_sentence_length,complex_word_percent,fog_index,average_number_words_per_sentence,complex_word_count,word_count,syllable_count_per_word,pronoun_count,average_word_length\n",
        "\n",
        "  # print(f\"text \\n {token_words} \\n\\n positive score {positive_score} \\n negative score {negative_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV_zgzo3omaM"
      },
      "source": [
        "in try block,\n",
        "  most of the of the article created inside the div of class name = \"td-post-content tagdiv-type\" where the main contents are present in the para tags so extracted that alone\n",
        "\n",
        "in except block,\n",
        "  some of the article starts with the images so in that case the contents are present inside the div of class name \"td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type\"\n",
        "  so the extracted the para tag contents alone\n",
        "\n",
        "if a link doesnt comes under either of the cases it will be considered the link doesnt working\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "kEof5gjZTdci"
      },
      "outputs": [],
      "source": [
        "def scrap_article_from_url(url):\n",
        "\n",
        "  global link\n",
        "\n",
        "  error_links = []\n",
        "\n",
        "  status = False\n",
        "\n",
        "  # url = \"https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-2/\"\n",
        "\n",
        "  page = requests.get(url)\n",
        "\n",
        "  soup = BeautifulSoup(page.text, \"html\")\n",
        "\n",
        "  title = soup.title.text\n",
        "\n",
        "  # after getting the entire information from the page get only the para tag contents\n",
        "\n",
        "  p_tags = \"\"\n",
        "\n",
        "  try:\n",
        "    div_tags = soup.find_all('div', class_ = \"td-post-content tagdiv-type\")\n",
        "\n",
        "    p_tags = div_tags[0].find_all('p')\n",
        "\n",
        "    status = True\n",
        "\n",
        "  except IndexError:\n",
        "    try:\n",
        "      div_tags = soup.find_all('div', class_ = \"td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type\")\n",
        "      p_tags = div_tags[0].find_all('div', class_ = \"tdb-block-inner td-fix-index\")\n",
        "      # print(div_tags)\n",
        "\n",
        "      p_tags = div_tags[0].find_all('p')\n",
        "      # print(p_tags)\n",
        "\n",
        "      status = True\n",
        "    except:\n",
        "      # error link - index of link that doesnt working properly\n",
        "      error_links.append(link)\n",
        "\n",
        "      status = False\n",
        "\n",
        "  # len(div_tags)\n",
        "\n",
        "  whole_content = \"\"\n",
        "\n",
        "\n",
        "  for i in p_tags:\n",
        "    whole_content += i.text\n",
        "\n",
        "  current_index = link\n",
        "  link += 1\n",
        "\n",
        "  print(current_index)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return status, whole_content, current_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4yb-Gd_VRSO"
      },
      "outputs": [],
      "source": [
        "# main programs starts here\n",
        "#index initially 0\n",
        "link = 0\n",
        "\n",
        "# loop throgh the each url\n",
        "for url in urls:\n",
        "  # text - article content\n",
        "  # index - current index\n",
        "  # status - is it ready to analyse or not\n",
        "  status, text, index = scrap_article_from_url(url)\n",
        "\n",
        "  # if the status is true then url is working properly\n",
        "  if status == True:\n",
        "    # result - contains the tuple of output variables\n",
        "    result = text_analysis(text, stop_words)\n",
        "    result = list(result) # type casted to list\n",
        "\n",
        "    # assign the values to their respective columns\n",
        "    for i in range(len(column_names)):\n",
        "      df.loc[index, column_names[i]] = result[i]\n",
        "\n",
        "\n",
        "df.to_csv(\"final_output.csv\", index = False)\n",
        "\n",
        "'''\n",
        " id : [\"blackassign0036\", \"blackassign0049\"]\n",
        " the above mentioned id and its respective url doesnt working so their entries remains zero(0)\n",
        "'''\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.3 32-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "b217b6f853589f9c26daf9e1d3992ae592c2ed06fb8c85b985301b466660b96d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
